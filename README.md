Pr√©diction de maladies cardiaques par Machine Learning
Objectif

Ce projet vise √† d√©velopper un mod√®le de machine learning capable de pr√©dire la pr√©sence d‚Äôune maladie cardiaque chez un patient √† partir de donn√©es m√©dicales. L‚Äôenjeu est de d√©tecter pr√©cocement les patients √† risque afin d‚Äôam√©liorer leur prise en charge. Pour cela, plusieurs algorithmes ont √©t√© entra√Æn√©s et compar√©s (r√©gression logistique, GLM, Random Forest, Random Forest optimis√©e, r√©seau de neurones, XGBoost) afin d‚Äôidentifier le mod√®le le plus performant.

Donn√©es

Le jeu de donn√©es provient de la base UCI Heart Disease, qui compile des informations cliniques de patients (√¢ge, sexe, type de douleur thoracique, pression art√©rielle, cholest√©rol, etc.). Chaque entr√©e repr√©sente un patient, et la variable cible HeartDisease indique si ce patient souffre d‚Äôune maladie cardiaque (1 = oui, 0 = non).

M√©thodologie

La m√©thodologie a mobilis√© plusieurs comp√©tences¬†: nettoyage des donn√©es (traitement des valeurs manquantes et des anomalies), analyse exploratoire (visualisation des distributions, corr√©lations) et s√©lection de variables pertinentes pour la mod√©lisation. Diff√©rents mod√®les ont ensuite √©t√© entra√Æn√©s, avec optimisation des hyperparam√®tres (ex. GridSearchCV pour la Random Forest) et √©valuation des performances via validation crois√©e et m√©triques (matrices de confusion, F1-score, AUC), puis visualisation des r√©sultats (courbes ROC, comparatif des mod√®les).

Technologies

D√©veloppement r√©alis√© en Python (Jupyter Notebook) avec les biblioth√®ques Pandas, NumPy, Scikit-learn, Statsmodels, TensorFlow/Keras, XGBoost et les outils de visualisation Matplotlib/Seaborn.

R√©sultats cl√©s

Comparaison des performances des mod√®les (F1-score en validation crois√©e et AUC ROC).
Le graphique ci-dessus compare le F1-score et l‚ÄôAUC des diff√©rents mod√®les. XGBoost ressort comme le meilleur mod√®le avec un F1-score de 0,88 et une AUC de 0,92. Il est suivi de pr√®s par le r√©seau de neurones (F1 ‚âà 0,85, AUC 0,90) et la Random Forest optimis√©e (F1 0,86, AUC 0,92), aux performances presque √©quivalentes. En comparaison, les mod√®les lin√©aires (r√©gression logistique et GLM) se r√©v√®lent moins performants (F1 ~0,75‚Äì0,79, AUC ~0,78).

üëâ **XGBoost** offre la meilleure performance globale, combinant robustesse, stabilit√© et pr√©cision.
